{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc41c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9aa4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __call__(self, x):\n",
    "        exps = np.exp(x)\n",
    "        self._softmax = exps / np.sum(exps)\n",
    "        return self._softmax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ddfc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        exps = np.exp(x)\n",
    "        self._sigmoid = exps / (1 + exps)\n",
    "        return self._sigmoid\n",
    "\n",
    "    def derivative(self):\n",
    "        return self._sigmoid * (1-self._sigmoid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e1f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __call__(self, preds, labels):\n",
    "        self._loss = np.square(preds - labels)\n",
    "        return np.mean(self._loss)\n",
    "    \n",
    "    def derivative(self):\n",
    "        return np.sqrt(self._loss) * (-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999f032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    exp_minus = np.exp(-1 * x)\n",
    "    exp_plus = np.exp(x)\n",
    "    return (exp_plus + exp_minus) / (exp_plus + exp_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44c54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __call__(self,x):\n",
    "        exp_minus = np.exp(-1 * x)\n",
    "        exp_plus = np.exp(x)\n",
    "        self.tanh = (exp_plus + exp_minus) / (exp_plus + exp_minus)\n",
    "        return self.tanh\n",
    "    \n",
    "    def derivative(self):\n",
    "        return (1+self.tanh) * (1-self.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba48c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = w*x + b \n",
    "#dy/dw =  x\n",
    "#dy/db = 1\n",
    "#loss = sum(out-y)**2 / len\n",
    "#dloss/dy = 1/2 * sum(out-y) * (-1) * len\n",
    "#dloss/dw = dloss/dy * dy/dw = -(sum(out-y)* len) /2 * x = -1 * root(loss) /2 * x\n",
    "#dloss/db = dloss/dy * dy/db = -sum(out-y) * len /2 * 1 = -1 * root(loss) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207972be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PReLU:\n",
    "    def __init__(self, a=0.25):\n",
    "        self.a = a\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        zeros = np.zeros(x.shape)\n",
    "        self._z = np.max([zeros, x], axis = 0) + self.a * np.min([zeros, x], axis = 0) \n",
    "        return self._z\n",
    "    \n",
    "    def derivative(self):\n",
    "        x, y = self._z.shape\n",
    "        zeros = np.zeros((x,y))\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                if self._z[i][j] >0:\n",
    "                    zeros[i][j] = 1\n",
    "                elif self._z[i][j] <0:\n",
    "                    zeros[i][j] =  -self.a\n",
    "        return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f345a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PReLU(x, a = 0.25):\n",
    "    zeros = np.zeros(x.shape)\n",
    "    return np.max([zeros, x], axis = 0) + a * np.min([zeros, x], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ad30c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 :  y = w*x + b \n",
    "#           loss = MSELoss(y', y)\n",
    "\n",
    "# 이후 :  y = w*x + b\n",
    "#            z = act_fn(y)\n",
    "#           loss = MSELoss(y', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, input_shape, hidden_dim, h=None, act_fn='tanh'):\n",
    "        self.batch_size, self.length, self.embedding_size = input_shape\n",
    "        self.weight, self.bias = [], []\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ####추가한 부분\n",
    "        if act_fn=='tanh':\n",
    "            self.act_fn = Tanh()\n",
    "            \n",
    "        elif act_fn=='sigmoid':\n",
    "            self.act_fn = Sigmoid()\n",
    "        ####\n",
    "        if h is None:\n",
    "            self.h = np.zeros((self.batch_size, self.length+1, hidden_dim)) \n",
    "        else:\n",
    "            self.h = h\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            self.in_size = self.embedding_size\n",
    "            w_h = np.random.rand(self.hidden_dim, self.hidden_dim)\n",
    "            w_x = np.random.rand(self.in_size, self.hidden_dim)\n",
    "            weight = np.concatenate((w_h, w_x), axis= 0) #(self.in_size+hidden_dim , hidden_dim )\n",
    "            self.weight.append(weight) \n",
    "            self.bias.append(np.random.rand(self.hidden_dim))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        next_x = x[0]\n",
    "        outs = []\n",
    "        for t in range(self.length):\n",
    "            out = np.matmul(np.concatenate((self.h[:,t,:], x[:,t,:]),axis=1),self.weight[t]) + self.bias[t] \n",
    "            self.h[:,t+1,:] = out[:self.hidden_dim,:] # 다음 스텝에서 h_(t)역할을 해줄 것.\n",
    "            real_out = out[self.hidden_dim:,:]\n",
    "            real_out = self.act_fn(out) #활성화함수\n",
    "            outs.append(real_out)\n",
    "        \n",
    "        outs = np.array(outs) # shape (length, batch_size, hidden_dim)\n",
    "        outs = np.transpose(outs, (1,0,2))\n",
    "        return self.h[:,1:], outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ca3e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[[1,2,3,4], [3,4,5,6], [4,5,6,7]], [[3,4,1,1], [4,5,1,1], [5,6,1,1]]]\n",
    "x = np.array(x)\n",
    "x.shape # ( 2,3,4) batch_size 2, timestamp_length 3, embedding_size 4로 만들어주었다. \n",
    "\n",
    "layer = SimpleRNN(x.shape, 8, h=None)\n",
    "layer(x)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28181053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_shape, hidden_dim, h=None, act_fn='Tanh'):\n",
    "        self.batch_size, self.length, self.embedding_size = input_shape\n",
    "        self.weight, self.bias = [], []\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ####추가한 부분\n",
    "        if act_fn=='Tanh':\n",
    "            self.act_fn = Tanh()\n",
    "            \n",
    "        elif act_fn=='Sigmoid':\n",
    "            self.act_fn = Sigmoid()\n",
    "        ####\n",
    "        if h is None:\n",
    "            self.h = np.zeros((self.batch_size, self.length+1, hidden_dim)) \n",
    "        else:\n",
    "            self.h = h\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            self.in_size = self.embedding_size\n",
    "            w_h = np.random.rand(self.hidden_dim, self.hidden_dim)\n",
    "            w_x = np.random.rand(self.in_size, self.hidden_dim)\n",
    "            weight = np.concatenate((w_h, w_x), axis= 0) #(self.in_size+hidden_dim , hidden_dim )\n",
    "            self.weight.append(weight) \n",
    "            self.bias.append(np.random.rand(self.hidden_dim))\n",
    "        \n",
    "    def __call__(self, x, h =None):\n",
    "        next_x = x[0]\n",
    "        if h is not None:\n",
    "            self.h = h\n",
    "        outs = []\n",
    "        for t in range(self.length):\n",
    "            #print(f'{t}th timestamp, x[t] shape {x[:,t,:].shape}, h[t] shape {self.h[:,t,:].shape}, self.weight[t] shape {self.weight[t].shape}')\n",
    "            new_x = np.concatenate((self.h[:,t,:], x[:,t,:]),axis=1)\n",
    "            #print(f'new_x shape {new_x.shape} bias shape {self.bias[t].shape}')\n",
    "            out = np.matmul(new_x,self.weight[t]) + self.bias[t] \n",
    "            self.h[:,t+1,:] = out[:self.hidden_dim,:] # 다음 스텝에서 h_(t)역할을 해줄 것.\n",
    "            real_out = out[self.hidden_dim:,:]\n",
    "            real_out = self.act_fn(out) #활성화함수\n",
    "            outs.append(real_out)\n",
    "        \n",
    "        outs = np.array(outs) # shape (length, batch_size, hidden_dim)\n",
    "        outs = np.transpose(outs, (1,0,2))\n",
    "        return outs\n",
    "    \n",
    "    #backpropagation 함수 추가. \n",
    "    def backpropagation(self, x, z, learning_rate):\n",
    "        dz_dy = self.act_fn.derivative() \n",
    "        for l in reversed(range(self.length)): #timestamp만큼의 RNN 노드가 있다. l번째 weight, bias를 순서대로 역전파 계산하자. \n",
    "            dy_dw = np.concatenate((self.h[:,l,:], x[:,l,:]), axis=-1) #(batch_size, 1, self.embedding_size + self.in_size)\n",
    "            #print(f'dy_dw shape {dy_dw.shape}')\n",
    "            dy_db = 1\n",
    "            dz_dw = np.matmul(np.transpose(dy_dw), dz_dy)\n",
    "            #print(f'dz_dw shape { dz_dw.shape}')\n",
    "            dz_db = dz_dy * dy_db\n",
    "            self.weight[l] = self.weight[l] + learning_rate * dz_dw\n",
    "            self.bias[l] = self.bias[l] + learning_rate * dz_db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c44ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = RNN(x.shape, 8, h=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ace466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[[1,2,3,4], [3,4,5,6], [4,5,6,7]], [[3,4,1,1], [4,5,1,1], [5,6,1,1]]]\n",
    "y = [[0], [1]]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489e66de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fcd316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, x, y, n_layers =3, n_node = 32, epochs=10, learning_rate=1e-3):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = []\n",
    "        self.loss_fcn = MSELoss()\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size, self.length, self.embedding_size = x.shape\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            act_fn = 'Tanh'\n",
    "            out_shape = n_node\n",
    "            \n",
    "            if i != 0: #첫번째 레이어\n",
    "                self.embedding_size = n_node\n",
    "               \n",
    "            if i == n_layers-1: #마지막 레이어\n",
    "                out_shape = y.shape[-1]\n",
    "                act_fn = 'Sigmoid'\n",
    "            \n",
    "            print(f'{i}th layer input_shape({self.batch_size, self.length,self.embedding_size},hidden dim {out_shape})')\n",
    "            self.layers.append(RNN(input_shape=(self.batch_size,self.length,self.embedding_size), hidden_dim=out_shape, h = None, act_fn=act_fn))\n",
    "\n",
    "        self._train(x, y)\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        outs = []\n",
    "        new_x = x\n",
    "        for layer in self.layers:\n",
    "            #이전 layer의 output값인 hidden layer를 넣어보자.\n",
    "            new_x  = layer(new_x)\n",
    "            outs.append(new_x)\n",
    "        \n",
    "        return outs, new_x #outs에는 지금까지 layer들의 출력값이 담겨있다.\n",
    "\n",
    "    def _backpropagation(self, x, outs, loss, learning_rate):\n",
    "        for i in reversed(range(self.n_layers)):\n",
    "            if i == 0:\n",
    "                x_in = x\n",
    "            else:  \n",
    "                x_in = outs[i-1]\n",
    "            if i == self.n_layers - 1:\n",
    "                outs[i] = outs[i] * np.mean(self.loss_fcn.derivative())\n",
    "            self.layers[i].backpropagation(x_in, outs[i], learning_rate)\n",
    "            \n",
    "        #    def backpropagation(self, x, z, learning_rate):\n",
    "\n",
    "    def _train(self, x, y):\n",
    "        for e in range(self.epochs):\n",
    "            outs, out = self._forward(x)\n",
    "            real_out = out[:,-1,:] #마지막 timestamp의 값을 가져온다. \n",
    "            loss = self.loss_fcn(real_out, y)\n",
    "            #outs.append(loss) #각 layer의 역전파를 위해 쓰일 배열이다. \n",
    "            self._backpropagation(x, outs, loss, self.learning_rate)\n",
    "        \n",
    "            print(f'{e+1}번째 epoch의 loss는 {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf2666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th layer input_shape((2, 3, 4),hidden dim 8)\n",
      "1th layer input_shape((2, 3, 8),hidden dim 8)\n",
      "2th layer input_shape((2, 3, 8),hidden dim 1)\n",
      "1번째 epoch의 loss는 0.4992922081357468\n",
      "2번째 epoch의 loss는 0.49929225584002224\n",
      "3번째 epoch의 loss는 0.49929230353860776\n",
      "4번째 epoch의 loss는 0.4992923512315045\n",
      "5번째 epoch의 loss는 0.49929239891871335\n",
      "6번째 epoch의 loss는 0.4992924466002353\n",
      "7번째 epoch의 loss는 0.49929249427607136\n",
      "8번째 epoch의 loss는 0.4992925419462224\n",
      "9번째 epoch의 loss는 0.49929258961068956\n",
      "10번째 epoch의 loss는 0.4992926372694737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Train at 0x7f89683ec2e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Train\n",
    "train(x,y, n_node=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145e4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f2389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
